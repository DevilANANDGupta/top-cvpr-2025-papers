![visitor badge](https://visitor-badge.laobi.icu/badge?page_id=SkalskiP.top-cvpr-2025-papers)

<div align="center">
  <h1 align="center">top CVPR 2025 papers</h1>
  <a href="https://github.com/SkalskiP/top-cvpr-2023-papers">2023</a> | <a href="https://github.com/SkalskiP/top-cvpr-2024-papers">2024</a> | <a href="https://github.com/SkalskiP/top-cvpr-2025-papers">2025</a>
</div>

## üëã hello

Computer Vision and Pattern Recognition is a massive conference. In **2025** alone,
**13,008** papers were submitted, and **2,878** were accepted. I created this repository
to help you search for cr√®me de la cr√®me of CVPR publications. If the paper you are
looking for is not on my short list, take a peek at the full
[list](https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers) of accepted papers.

## üóûÔ∏è papers and posters

*üî• - highlighted papers*

<!--- AUTOGENERATED_PAPERS_LIST -->
<!---
   WARNING: DO NOT EDIT THIS LIST MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO https://github.com/SkalskiP/top-cvpr-2024-papers/blob/master/CONTRIBUTING.md FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
-->
### 3d vision

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33969.png?t=1748740040.9726639" title="VGGT: Visual Geometry Grounded Transformer">
        <img src="https://github.com/user-attachments/assets/2f73ac33-7ddd-4f74-a020-db9a7e49c74c" alt="VGGT: Visual Geometry Grounded Transformer" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.11651" title="VGGT: Visual Geometry Grounded Transformer">
        <strong>üî• VGGT: Visual Geometry Grounded Transformer</strong>
    </a>
    <br/>
    Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny
    <br/>
    [<a href="https://arxiv.org/abs/2503.11651">paper</a>] [<a href="https://github.com/facebookresearch/vggt">code</a>] [<a href="https://youtu.be/7ZYwJEpCUUA">video</a>] [<a href="https://huggingface.co/spaces/facebook/vggt">demo</a>] 
    <br/>
    <strong>Topic:</strong> 3D Vision
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST Poster Session 2 #86
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34871.png?t=1748708079.0490072" title="MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors">
        <img src="https://github.com/user-attachments/assets/ec55204b-ad3f-4571-bd89-5efc534035ab" alt="MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.12392" title="MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors">
        <strong>üî• MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</strong>
    </a>
    <br/>
    Riku Murai, Eric Dexheimer, Andrew J. Davison
    <br/>
    [<a href="https://arxiv.org/abs/2412.12392">paper</a>] [<a href="https://github.com/rmurai0610/MASt3R-SLAM">code</a>] [<a href="https://www.youtube.com/watch?v=wozt71NBFTQ">video</a>]  
    <br/>
    <strong>Topic:</strong> 3D Vision
    <br/>
    <strong>Session:</strong> Sun 15 Jun midnight CEST ‚Äî 2 a.m. CEST Poster Session 4 #83
</p>
<br/>
<br/>

### depth estimation

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32554.png?t=1748195633.2703488" title="UniK3D: Universal Camera Monocular 3D Estimation">
        <img src="https://github.com/user-attachments/assets/e91f48d0-5105-415c-b2cf-c37ff1d1a907" alt="UniK3D: Universal Camera Monocular 3D Estimation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.16591" title="UniK3D: Universal Camera Monocular 3D Estimation">
        <strong>UniK3D: Universal Camera Monocular 3D Estimation</strong>
    </a>
    <br/>
    Luigi Piccinelli, Christos Sakaridis, Mattia Segu, Yung-Hsu Yang, Siyuan Li, Wim Abbeloos, Luc Van Gool
    <br/>
    [<a href="https://arxiv.org/abs/2503.16591">paper</a>] [<a href="https://github.com/lpiccinelli-eth/UniK3D">code</a>]  [<a href="https://huggingface.co/spaces/lpiccinelli/UniK3D-demo">demo</a>] 
    <br/>
    <strong>Topic:</strong> Depth Estimation
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 1 #80
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33026.png?t=1749131392.5907311" title="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos">
        <img src="https://github.com/user-attachments/assets/fabe851e-1533-4752-8875-44640da9dc3f" alt="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2409.02095" title="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos">
        <strong>üî• DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</strong>
    </a>
    <br/>
    Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan
    <br/>
    [<a href="https://arxiv.org/abs/2409.02095">paper</a>] [<a href="https://github.com/Tencent/DepthCrafter">code</a>]  [<a href="https://huggingface.co/spaces/tencent/DepthCrafter">demo</a>] 
    <br/>
    <strong>Topic:</strong> Depth Estimation
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 1 #171
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33862.png?t=1747997885.3577623" title="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos">
        <img src="https://github.com/user-attachments/assets/81fd3218-7db3-4446-9009-11489742120d" alt="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2501.12375" title="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos">
        <strong>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</strong>
    </a>
    <br/>
    Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang
    <br/>
    [<a href="https://arxiv.org/abs/2501.12375">paper</a>] [<a href="https://github.com/DepthAnything/Video-Depth-Anything">code</a>]  [<a href="https://huggingface.co/spaces/depth-anything/Video-Depth-Anything">demo</a>] 
    <br/>
    <strong>Topic:</strong> Depth Estimation
    <br/>
    <strong>Session:</strong> Sun 15 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 5 #169
</p>
<br/>
<br/>

### explainability and interpretability

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34644.png?t=1748853080.0173087" title="Interpreting Object-level Foundation Models via Visual Precision Search">
        <img src="https://github.com/user-attachments/assets/4ea7bae6-1064-44a7-8e82-8ce1051acef1" alt="Interpreting Object-level Foundation Models via Visual Precision Search" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.16198" title="Interpreting Object-level Foundation Models via Visual Precision Search">
        <strong>üî• Interpreting Object-level Foundation Models via Visual Precision Search</strong>
    </a>
    <br/>
    Ruoyu Chen, Siyuan Liang, Jingzhi Li, Shiming Liu, Maosen Li, Zhen Huang, Hua Zhang, Xiaochun Cao
    <br/>
    [<a href="https://arxiv.org/abs/2411.16198">paper</a>] [<a href="https://github.com/RuoyuChen10/VPS">code</a>]   [<a href="https://colab.research.google.com/github/RuoyuChen10/VPS/blob/main/tutorial/Grounding_DINO_explanation.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Explainability and Interpretability
    <br/>
    <strong>Session:</strong> Sun 15 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST Poster Session 6 #372
</p>
<br/>
<br/>

### generative models

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33479.png?t=1748410877.7850628" title="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis">
        <img src="https://github.com/user-attachments/assets/f235f59f-98d7-4dcf-b211-a4b47c548bf1" alt="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.15322" title="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis">
        <strong>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</strong>
    </a>
    <br/>
    Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, Yuki Mitsufuji
    <br/>
    [<a href="https://arxiv.org/abs/2412.15322">paper</a>] [<a href="https://github.com/hkchengrex/MMAudio">code</a>] [<a href="https://youtu.be/YElewUT2M4M">video</a>] [<a href="https://huggingface.co/spaces/hkchengrex/MMAudio">demo</a>] [<a href="https://colab.research.google.com/drive/1TAaXCY2-kPk4xE4PwKB3EqFbSnkUuzZ8?usp=sharing">colab</a>]
    <br/>
    <strong>Topic:</strong> Generative Models
    <br/>
    <strong>Session:</strong> Sun 15 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST Poster Session 6 #260
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32785.png?t=1748780647.7372541" title="SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models">
        <img src="https://github.com/user-attachments/assets/4e57fcf0-2833-4fb6-88f4-7ee6aeb94e7d" alt="SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2403.09055" title="SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models">
        <strong>SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models</strong>
    </a>
    <br/>
    Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee
    <br/>
    [<a href="https://arxiv.org/abs/2403.09055">paper</a>] [<a href="https://github.com/ironjr/semantic-draw">code</a>] [<a href="https://www.youtube.com/watch?v=qR06iiaG5nc">video</a>] [<a href="https://huggingface.co/spaces/ironjr/semantic-draw-canvas-sdxl">demo</a>] [<a href="https://colab.research.google.com/github/camenduru/SemanticPalette-jupyter/blob/main/SemanticPalette_jupyter.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Generative Models
    <br/>
    <strong>Session:</strong> Sat 14 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 3 #226
</p>
<br/>
<br/>

### image matching

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34813.png?t=1748519375.4086587" title="MINIMA: Modality Invariant Image Matching">
        <img src="https://github.com/user-attachments/assets/74fb83da-9361-45da-977f-22c9d4808111" alt="MINIMA: Modality Invariant Image Matching" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.19412" title="MINIMA: Modality Invariant Image Matching">
        <strong>MINIMA: Modality Invariant Image Matching</strong>
    </a>
    <br/>
    Jiangwei Ren, Xingyu Jiang, Zizhuo Li, Dingkang Liang, Xin Zhou, Xiang Bai
    <br/>
    [<a href="https://arxiv.org/abs/2412.19412">paper</a>] [<a href="https://github.com/LSXI7/MINIMA">code</a>]  [<a href="https://huggingface.co/spaces/lsxi77777/MINIMA">demo</a>] 
    <br/>
    <strong>Topic:</strong> Image Matching
    <br/>
    <strong>Session:</strong> Sun 15 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 5 #190
</p>
<br/>
<br/>

### image vectorization

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34467.png?t=1748720373.8849306" title="Layered Image Vectorization via Semantic Simplification">
        <img src="https://github.com/user-attachments/assets/29830750-91b8-42cb-a7c9-c30f917abba3" alt="Layered Image Vectorization via Semantic Simplification" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2406.05404" title="Layered Image Vectorization via Semantic Simplification">
        <strong>Layered Image Vectorization via Semantic Simplification</strong>
    </a>
    <br/>
    Zhenyu Wang, Jianxi Huang, Zhida Sun, Yuanhao Gong, Daniel Cohen-Or, Min Lu
    <br/>
    [<a href="https://arxiv.org/abs/2406.05404">paper</a>] [<a href="https://github.com/SZUVIZ/layered_vectorization">code</a>] [<a href="https://youtu.be/oUYQKjDlwCw">video</a>]  
    <br/>
    <strong>Topic:</strong> Image Vectorization
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST Poster Session 2 #226
</p>
<br/>
<br/>

### object detection

<p align="left">
    <a href="https://arxiv.org/abs/2412.04234" title="DEIM: DETR with Improved Matching for Fast Convergence">
        <strong>DEIM: DETR with Improved Matching for Fast Convergence</strong>
    </a>
    <br/>
    Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, Xi Shen
    <br/>
    [<a href="https://arxiv.org/abs/2412.04234">paper</a>] [<a href="https://github.com/ShihuaHuang95/DEIM">code</a>]   
    <br/>
    <strong>Topic:</strong> Object Detection
    <br/>
    <strong>Session:</strong> Sat 14 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 3 #432
</p>
<br/>

### object tracking

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/35010.png?t=1748684123.2752578" title="MITracker: Multi-View Integration for Visual Object Tracking">
        <img src="https://github.com/user-attachments/assets/80de6a58-6ffd-477c-b71e-6e36f613c454" alt="MITracker: Multi-View Integration for Visual Object Tracking" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2502.20111" title="MITracker: Multi-View Integration for Visual Object Tracking">
        <strong>üî• MITracker: Multi-View Integration for Visual Object Tracking</strong>
    </a>
    <br/>
    Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang
    <br/>
    [<a href="https://arxiv.org/abs/2502.20111">paper</a>] [<a href="https://github.com/XuM007/MITracker">code</a>]   
    <br/>
    <strong>Topic:</strong> Object Tracking
    <br/>
    <strong>Session:</strong> Sun 15 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST Poster Session 6 #98
</p>
<br/>
<br/>

### open-world detection

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/35020.png?t=1748563484.5053573" title="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models">
        <img src="https://github.com/user-attachments/assets/40164060-591d-4cc0-a48c-2685beed2396" alt="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2502.07601" title="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models">
        <strong>üî• Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models</strong>
    </a>
    <br/>
    Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi
    <br/>
    [<a href="https://arxiv.org/abs/2502.07601">paper</a>] [<a href="https://github.com/honda-research-institute/Anomaly-OneVision">code</a>] [<a href="https://www.youtube.com/watch?v=b3-qGTm23eA">video</a>]  
    <br/>
    <strong>Topic:</strong> Open-World Detection
    <br/>
    <strong>Session:</strong> Sun 15 Jun midnight CEST ‚Äî 2 a.m. CEST Poster Session 4 #435
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32802.png?t=1748865568.2500262" title="Compositional Caching for Training-free Open-vocabulary Attribute Detection">
        <img src="https://github.com/user-attachments/assets/d0696a83-3aa0-417d-92a8-2624ab030e5c" alt="Compositional Caching for Training-free Open-vocabulary Attribute Detection" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.19145" title="Compositional Caching for Training-free Open-vocabulary Attribute Detection">
        <strong>üî• Compositional Caching for Training-free Open-vocabulary Attribute Detection</strong>
    </a>
    <br/>
    Marco Garosi, Alessandro Conti, Gaowen Liu, Elisa Ricci, Massimiliano Mancini
    <br/>
    [<a href="https://arxiv.org/abs/2503.19145">paper</a>] [<a href="https://github.com/marco-garosi/ComCa">code</a>] [<a href="https://youtu.be/ruHSAGemMa8">video</a>]  
    <br/>
    <strong>Topic:</strong> Open-World Detection
    <br/>
    <strong>Session:</strong> Sat 14 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 3 #426
</p>
<br/>
<br/>

### pose estimation

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/35057.png?t=1748706748.0220559" title="Reconstructing Humans with a Biomechanically Accurate Skeleton">
        <img src="https://github.com/user-attachments/assets/2bd230a4-9ea3-4a78-8daf-b9511c0c0baa" alt="Reconstructing Humans with a Biomechanically Accurate Skeleton" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.21751" title="Reconstructing Humans with a Biomechanically Accurate Skeleton">
        <strong>üî• Reconstructing Humans with a Biomechanically Accurate Skeleton</strong>
    </a>
    <br/>
    Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos
    <br/>
    [<a href="https://arxiv.org/abs/2503.21751">paper</a>] [<a href="https://github.com/IsshikiHugh/HSMR">code</a>]  [<a href="https://huggingface.co/spaces/IsshikiHugh/HSMR">demo</a>] [<a href="https://colab.research.google.com/drive/1RDA9iKckCDKh_bbaKjO8bQ0-Lv5fw1CB?usp=sharing">colab</a>]
    <br/>
    <strong>Topic:</strong> Pose Estimation
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST Poster Session 2 #91
</p>
<br/>
<br/>

### segmentation

<p align="left">
    <a href="https://arxiv.org/abs/2501.14677" title="MatAnyone: Stable Video Matting with Consistent Memory Propagation">
        <strong>MatAnyone: Stable Video Matting with Consistent Memory Propagation</strong>
    </a>
    <br/>
    Peiqing Yang, Shangchen Zhou, Jixin Zhao, Qingyi Tao, Chen Change Loy
    <br/>
    [<a href="https://arxiv.org/abs/2501.14677">paper</a>] [<a href="https://github.com/pq-yang/MatAnyone">code</a>] [<a href="https://www.youtube.com/watch?v=oih0Zk-UW18">video</a>] [<a href="https://huggingface.co/spaces/PeiqingYang/MatAnyone">demo</a>] 
    <br/>
    <strong>Topic:</strong> Segmentation
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST Poster Session 2 #185
</p>
<br/>

### stereo matching

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34330.png?t=1748714664.9139624" title="FoundationStereo: Zero-Shot Stereo Matching">
        <img src="https://github.com/user-attachments/assets/7b56609f-74c0-4019-91dd-ed3ea260875f" alt="FoundationStereo: Zero-Shot Stereo Matching" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2501.09898" title="FoundationStereo: Zero-Shot Stereo Matching">
        <strong>üî• FoundationStereo: Zero-Shot Stereo Matching</strong>
    </a>
    <br/>
    Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield
    <br/>
    [<a href="https://arxiv.org/abs/2501.09898">paper</a>] [<a href="https://github.com/NVlabs/FoundationStereo">code</a>] [<a href="https://www.youtube.com/watch?v=R7RgHxEXB3o">video</a>]  
    <br/>
    <strong>Topic:</strong> Stereo Matching
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST Poster Session 2 #81
</p>
<br/>
<br/>

### vision-language models

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32887.png?t=1747896029.4399107" title="FastVLM: Efficient Vision Encoding for Vision Language Models">
        <img src="https://github.com/user-attachments/assets/6915d6e4-7219-4264-ba66-f947c36a1573" alt="FastVLM: Efficient Vision Encoding for Vision Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.13303" title="FastVLM: Efficient Vision Encoding for Vision Language Models">
        <strong>FastVLM: Efficient Vision Encoding for Vision Language Models</strong>
    </a>
    <br/>
    Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari
    <br/>
    [<a href="https://arxiv.org/abs/2412.13303">paper</a>] [<a href="https://github.com/apple/ml-fastvlm">code</a>]   
    <br/>
    <strong>Topic:</strong> Vision-Language Models
    <br/>
    <strong>Session:</strong> Sun 15 Jun midnight CEST ‚Äî 2 a.m. CEST Poster Session 4 #378
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33073.png?t=1748883064.876014" title="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models">
        <img src="https://github.com/user-attachments/assets/79e45954-fad5-4d21-a5ab-14a1d5b4b942" alt="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2409.17146" title="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models">
        <strong>üî• Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</strong>
    </a>
    <br/>
    Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
    <br/>
    [<a href="https://arxiv.org/abs/2409.17146">paper</a>]   [<a href="https://huggingface.co/spaces/akhaliq/Molmo-7B-D-0924">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision-Language Models
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 1 #80
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34510.png?t=1748805761.17" title="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation">
        <img src="https://github.com/user-attachments/assets/57d90a0c-3ab6-497f-83c0-ed4a46a669ac" alt="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.17646" title="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation">
        <strong>üî• SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation</strong>
    </a>
    <br/>
    Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, Giuseppe Averta
    <br/>
    [<a href="https://arxiv.org/abs/2411.17646">paper</a>] [<a href="https://github.com/ClaudiaCuttano/SAMWISE">code</a>] [<a href="https://youtu.be/OL3xvzFyXCc">video</a>]  
    <br/>
    <strong>Topic:</strong> Vision-Language Models
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 1 #308
</p>
<br/>
<br/>

### visual agents

<p align="left">
    <a href="https://arxiv.org/abs/2502.13130" title="Magma: A Foundation Model for Multimodal AI Agents">
        <strong>Magma: A Foundation Model for Multimodal AI Agents</strong>
    </a>
    <br/>
    Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, Jianfeng Gao
    <br/>
    [<a href="https://arxiv.org/abs/2502.13130">paper</a>] [<a href="https://github.com/microsoft/Magma">code</a>] [<a href="https://www.youtube.com/watch?v=SbfzvUU5yM8">video</a>] [<a href="https://huggingface.co/spaces/microsoft/Magma-UI">demo</a>] 
    <br/>
    <strong>Topic:</strong> Visual Agents
    <br/>
    <strong>Session:</strong> Sat 14 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST Poster Session 3 #340
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33472.png?t=1748798588.1133444" title="ShowUI: One Vision-Language-Action Model for GUI Visual Agent">
        <img src="https://github.com/user-attachments/assets/6dd848ca-51da-479e-9edc-e4f3efc34fc8" alt="ShowUI: One Vision-Language-Action Model for GUI Visual Agent" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.17465" title="ShowUI: One Vision-Language-Action Model for GUI Visual Agent">
        <strong>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</strong>
    </a>
    <br/>
    Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, Mike Zheng Shou
    <br/>
    [<a href="https://arxiv.org/abs/2411.17465">paper</a>] [<a href="https://github.com/showlab/ShowUI">code</a>]  [<a href="https://huggingface.co/spaces/showlab/ShowUI">demo</a>] 
    <br/>
    <strong>Topic:</strong> Visual Agents
    <br/>
    <strong>Session:</strong> Sun 15 Jun midnight CEST ‚Äî 2 a.m. CEST Poster Session 4 #352
</p>
<br/>
<br/>

<!--- AUTOGENERATED_PAPERS_LIST -->

## ü¶∏ contribution

We would love your help in making this repository even better! If you know of an amazing
paper that isn't listed here, or if you have any suggestions for improvement, feel free
to open an
[issue](https://github.com/SkalskiP/top-cvpr-2025-papers/issues)
or submit a
[pull request](https://github.com/SkalskiP/top-cvpr-2025-papers/pulls).
