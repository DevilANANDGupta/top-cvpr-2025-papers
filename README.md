![visitor badge](https://visitor-badge.laobi.icu/badge?page_id=SkalskiP.top-cvpr-2025-papers)

<div align="center">
  <h1 align="center">top CVPR 2025 papers</h1>
  <a href="https://github.com/SkalskiP/top-cvpr-2023-papers">2023</a> | <a href="https://github.com/SkalskiP/top-cvpr-2024-papers">2024</a> | <a href="https://github.com/SkalskiP/top-cvpr-2025-papers">2025</a>
</div>

## üëã hello

Computer Vision and Pattern Recognition is a massive conference. In **2025** alone,
**13,008** papers were submitted, and **2,878** were accepted. I created this repository
to help you search for cr√®me de la cr√®me of CVPR publications. If the paper you are
looking for is not on my short list, take a peek at the full
[list](https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers) of accepted papers.

## üóûÔ∏è papers and posters

*üî• - highlighted papers*

<!--- AUTOGENERATED_PAPERS_LIST -->
<!---
   WARNING: DO NOT EDIT THIS LIST MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO https://github.com/SkalskiP/top-cvpr-2025-papers/blob/master/CONTRIBUTING.md FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
-->

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32554.png?t=1748195633.2703488" title="UniK3D: Universal Camera Monocular 3D Estimation">
        <img src="https://github.com/user-attachments/assets/e91f48d0-5105-415c-b2cf-c37ff1d1a907" alt="UniK3D: Universal Camera Monocular 3D Estimation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.16591" title="UniK3D: Universal Camera Monocular 3D Estimation">
        <strong>UniK3D: Universal Camera Monocular 3D Estimation</strong>
    </a>
    <br/>
    Luigi Piccinelli, Christos Sakaridis, Mattia Segu, Yung-Hsu Yang, Siyuan Li, Wim Abbeloos, Luc Van Gool
    <br/>
    [<a href="https://arxiv.org/abs/2503.16591">paper</a>] [<a href="https://github.com/lpiccinelli-eth/UniK3D">code</a>]  [<a href="https://huggingface.co/spaces/lpiccinelli/UniK3D-demo">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #80
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32887.png?t=1747896029.4399107" title="FastVLM: Efficient Vision Encoding for Vision Language Models">
        <img src="https://github.com/user-attachments/assets/6915d6e4-7219-4264-ba66-f947c36a1573" alt="FastVLM: Efficient Vision Encoding for Vision Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.13303" title="FastVLM: Efficient Vision Encoding for Vision Language Models">
        <strong>FastVLM: Efficient Vision Encoding for Vision Language Models</strong>
    </a>
    <br/>
    Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari
    <br/>
    [<a href="https://arxiv.org/abs/2412.13303">paper</a>] [<a href="https://github.com/apple/ml-fastvlm">code</a>]   
    <br/>
    <strong>Session:</strong> Sun 15 Jun midnight CEST ‚Äî 2 a.m. CEST
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33479.png?t=1748410877.7850628" title="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis">
        <img src="https://github.com/user-attachments/assets/f235f59f-98d7-4dcf-b211-a4b47c548bf1" alt="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.15322" title="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis">
        <strong>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</strong>
    </a>
    <br/>
    Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, Yuki Mitsufuji
    <br/>
    [<a href="https://arxiv.org/abs/2412.15322">paper</a>] [<a href="https://github.com/hkchengrex/MMAudio">code</a>] [<a href="https://youtu.be/YElewUT2M4M">video</a>] [<a href="https://huggingface.co/spaces/hkchengrex/MMAudio">demo</a>] [<a href="https://colab.research.google.com/drive/1TAaXCY2-kPk4xE4PwKB3EqFbSnkUuzZ8?usp=sharing">colab</a>]
    <br/>
    <strong>Session:</strong> Sun 15 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST #260
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33073.png?t=1748883064.876014" title="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models">
        <img src="https://github.com/user-attachments/assets/79e45954-fad5-4d21-a5ab-14a1d5b4b942" alt="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2409.17146" title="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models">
        <strong>üèÜ Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</strong>
    </a>
    <br/>
    Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
    <br/>
    [<a href="https://arxiv.org/abs/2409.17146">paper</a>]   [<a href="https://huggingface.co/spaces/akhaliq/Molmo-7B-D-0924">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #80
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34813.png?t=1748519375.4086587" title="MINIMA: Modality Invariant Image Matching">
        <img src="https://github.com/user-attachments/assets/74fb83da-9361-45da-977f-22c9d4808111" alt="MINIMA: Modality Invariant Image Matching" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.19412" title="MINIMA: Modality Invariant Image Matching">
        <strong>MINIMA: Modality Invariant Image Matching</strong>
    </a>
    <br/>
    Jiangwei Ren, Xingyu Jiang, Zizhuo Li, Dingkang Liang, Xin Zhou, Xiang Bai
    <br/>
    [<a href="https://arxiv.org/abs/2412.19412">paper</a>] [<a href="https://github.com/LSXI7/MINIMA">code</a>]  [<a href="https://huggingface.co/spaces/lsxi77777/MINIMA">demo</a>] 
    <br/>
    <strong>Session:</strong> Sun 15 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #190
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2412.04234" title="DEIM: DETR with Improved Matching for Fast Convergence">
        <strong>DEIM: DETR with Improved Matching for Fast Convergence</strong>
    </a>
    <br/>
    Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, Xi Shen
    <br/>
    [<a href="https://arxiv.org/abs/2412.04234">paper</a>] [<a href="https://github.com/ShihuaHuang95/DEIM">code</a>]   
    <br/>
    <strong>Session:</strong> Sat 14 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33969.png?t=1748740040.9726639" title="VGGT: Visual Geometry Grounded Transformer">
        <img src="https://github.com/user-attachments/assets/2f73ac33-7ddd-4f74-a020-db9a7e49c74c" alt="VGGT: Visual Geometry Grounded Transformer" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.11651" title="VGGT: Visual Geometry Grounded Transformer">
        <strong>üèÜ VGGT: Visual Geometry Grounded Transformer</strong>
    </a>
    <br/>
    Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny
    <br/>
    [<a href="https://arxiv.org/abs/2503.11651">paper</a>] [<a href="https://github.com/facebookresearch/vggt">code</a>] [<a href="https://youtu.be/7ZYwJEpCUUA">video</a>] [<a href="https://huggingface.co/spaces/facebook/vggt">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2502.13130" title="Magma: A Foundation Model for Multimodal AI Agents">
        <strong>Magma: A Foundation Model for Multimodal AI Agents</strong>
    </a>
    <br/>
    Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, Jianfeng Gao
    <br/>
    [<a href="https://arxiv.org/abs/2502.13130">paper</a>] [<a href="https://github.com/microsoft/Magma">code</a>] [<a href="https://www.youtube.com/watch?v=SbfzvUU5yM8">video</a>] [<a href="https://huggingface.co/spaces/microsoft/Magma-UI">demo</a>] 
    <br/>
    <strong>Session:</strong> Sat 14 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST
</p>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2501.14677" title="MatAnyone: Stable Video Matting with Consistent Memory Propagation">
        <strong>MatAnyone: Stable Video Matting with Consistent Memory Propagation</strong>
    </a>
    <br/>
    Peiqing Yang, Shangchen Zhou, Jixin Zhao, Qingyi Tao, Chen Change Loy
    <br/>
    [<a href="https://arxiv.org/abs/2501.14677">paper</a>] [<a href="https://github.com/pq-yang/MatAnyone">code</a>] [<a href="https://www.youtube.com/watch?v=oih0Zk-UW18">video</a>] [<a href="https://huggingface.co/spaces/PeiqingYang/MatAnyone">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33026.png?t=1749131392.5907311" title="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos">
        <img src="https://github.com/user-attachments/assets/fabe851e-1533-4752-8875-44640da9dc3f" alt="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2409.02095" title="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos">
        <strong>üèÜ DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</strong>
    </a>
    <br/>
    Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan
    <br/>
    [<a href="https://arxiv.org/abs/2409.02095">paper</a>] [<a href="https://github.com/Tencent/DepthCrafter">code</a>]  [<a href="https://huggingface.co/spaces/tencent/DepthCrafter">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #171
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33862.png?t=1747997885.3577623" title="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos">
        <img src="https://github.com/user-attachments/assets/81fd3218-7db3-4446-9009-11489742120d" alt="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2501.12375" title="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos">
        <strong>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</strong>
    </a>
    <br/>
    Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang
    <br/>
    [<a href="https://arxiv.org/abs/2501.12375">paper</a>] [<a href="https://github.com/DepthAnything/Video-Depth-Anything">code</a>]  [<a href="https://huggingface.co/spaces/depth-anything/Video-Depth-Anything">demo</a>] 
    <br/>
    <strong>Session:</strong> Sun 15 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #169
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33472.png?t=1748798588.1133444" title="ShowUI: One Vision-Language-Action Model for GUI Visual Agent">
        <img src="https://github.com/user-attachments/assets/6dd848ca-51da-479e-9edc-e4f3efc34fc8" alt="ShowUI: One Vision-Language-Action Model for GUI Visual Agent" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.17465" title="ShowUI: One Vision-Language-Action Model for GUI Visual Agent">
        <strong>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</strong>
    </a>
    <br/>
    Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, Mike Zheng Shou
    <br/>
    [<a href="https://arxiv.org/abs/2411.17465">paper</a>] [<a href="https://github.com/showlab/ShowUI">code</a>]  [<a href="https://huggingface.co/spaces/showlab/ShowUI">demo</a>] 
    <br/>
    <strong>Session:</strong> Sun 15 Jun midnight CEST ‚Äî 2 a.m. CEST #352
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34510.png?t=1748805761.17" title="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation">
        <img src="https://github.com/user-attachments/assets/57d90a0c-3ab6-497f-83c0-ed4a46a669ac" alt="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.17646" title="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation">
        <strong>üèÜ SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation</strong>
    </a>
    <br/>
    Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, Giuseppe Averta
    <br/>
    [<a href="https://arxiv.org/abs/2411.17646">paper</a>] [<a href="https://github.com/ClaudiaCuttano/SAMWISE">code</a>] [<a href="https://youtu.be/OL3xvzFyXCc">video</a>]  
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #308
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34467.png?t=1748720373.8849306" title="Layered Image Vectorization via Semantic Simplification">
        <img src="https://github.com/user-attachments/assets/29830750-91b8-42cb-a7c9-c30f917abba3" alt="Layered Image Vectorization via Semantic Simplification" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2406.05404" title="Layered Image Vectorization via Semantic Simplification">
        <strong>Layered Image Vectorization via Semantic Simplification</strong>
    </a>
    <br/>
    Zhenyu Wang, Jianxi Huang, Zhida Sun, Yuanhao Gong, Daniel Cohen-Or, Min Lu
    <br/>
    [<a href="https://arxiv.org/abs/2406.05404">paper</a>] [<a href="https://github.com/SZUVIZ/layered_vectorization">code</a>] [<a href="https://youtu.be/oUYQKjDlwCw">video</a>]  
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST #226
</p>
<br/>
<br/>

<!--- AUTOGENERATED_PAPERS_LIST -->

## ü¶∏ contribution

We would love your help in making this repository even better! If you know of an amazing
paper that isn't listed here, or if you have any suggestions for improvement, feel free
to open an
[issue](https://github.com/SkalskiP/top-cvpr-2025-papers/issues)
or submit a
[pull request](https://github.com/SkalskiP/top-cvpr-2025-papers/pulls).
