![visitor badge](https://visitor-badge.laobi.icu/badge?page_id=SkalskiP.top-cvpr-2025-papers)

<div align="center">
  <h1 align="center">top CVPR 2025 papers</h1>
  <a href="https://github.com/SkalskiP/top-cvpr-2023-papers">2023</a> | <a href="https://github.com/SkalskiP/top-cvpr-2024-papers">2024</a> | <a href="https://github.com/SkalskiP/top-cvpr-2025-papers">2025</a>
</div>

## üëã hello

Computer Vision and Pattern Recognition is a massive conference. In **2025** alone,
**13,008** papers were submitted, and **2,878** were accepted. I created this repository
to help you search for cr√®me de la cr√®me of CVPR publications. If the paper you are
looking for is not on my short list, take a peek at the full
[list](https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers) of accepted papers.

## üóûÔ∏è papers and posters

*üî• - highlighted papers*

<!--- AUTOGENERATED_PAPERS_LIST -->
<!---
   WARNING: DO NOT EDIT THIS LIST MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO https://github.com/SkalskiP/top-cvpr-2024-papers/blob/master/CONTRIBUTING.md FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
-->
### 3d vision

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33969.png?t=1748740040.9726639" title="VGGT: Visual Geometry Grounded Transformer">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/33969.png" alt="VGGT: Visual Geometry Grounded Transformer" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.11651" title="VGGT: Visual Geometry Grounded Transformer">
        <strong>üî• VGGT: Visual Geometry Grounded Transformer</strong>
    </a>
    <br/>
    Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny
    <br/>
    [<a href="https://arxiv.org/abs/2503.11651">paper</a>] [<a href="https://github.com/facebookresearch/vggt">code</a>] [<a href="https://youtu.be/7ZYwJEpCUUA">video</a>] [<a href="https://huggingface.co/spaces/facebook/vggt">demo</a>] 
    <br/>
    <strong>Topic:</strong> 3D Vision
    <br/>
    <strong>Session:</strong> Fri 13 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 2 #86
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34871.png?t=1748708079.0490072" title="MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/34871.png" alt="MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.12392" title="MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors">
        <strong>üî• MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors</strong>
    </a>
    <br/>
    Riku Murai, Eric Dexheimer, Andrew J. Davison
    <br/>
    [<a href="https://arxiv.org/abs/2412.12392">paper</a>] [<a href="https://github.com/rmurai0610/MASt3R-SLAM">code</a>] [<a href="https://www.youtube.com/watch?v=wozt71NBFTQ">video</a>]  
    <br/>
    <strong>Topic:</strong> 3D Vision
    <br/>
    <strong>Session:</strong> Sat 14 Jun 3 p.m. PDT ‚Äî 5 p.m. PDT Poster Session 4 #83
</p>
<br/>
<br/>

### depth estimation

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32554.png?t=1748195633.2703488" title="UniK3D: Universal Camera Monocular 3D Estimation">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/32554.png" alt="UniK3D: Universal Camera Monocular 3D Estimation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.16591" title="UniK3D: Universal Camera Monocular 3D Estimation">
        <strong>UniK3D: Universal Camera Monocular 3D Estimation</strong>
    </a>
    <br/>
    Luigi Piccinelli, Christos Sakaridis, Mattia Segu, Yung-Hsu Yang, Siyuan Li, Wim Abbeloos, Luc Van Gool
    <br/>
    [<a href="https://arxiv.org/abs/2503.16591">paper</a>] [<a href="https://github.com/lpiccinelli-eth/UniK3D">code</a>]  [<a href="https://huggingface.co/spaces/lpiccinelli/UniK3D-demo">demo</a>] 
    <br/>
    <strong>Topic:</strong> Depth Estimation
    <br/>
    <strong>Session:</strong> Fri 13 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 1 #80
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33026.png?t=1749131392.5907311" title="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/33026.png" alt="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2409.02095" title="DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos">
        <strong>üî• DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos</strong>
    </a>
    <br/>
    Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, Ying Shan
    <br/>
    [<a href="https://arxiv.org/abs/2409.02095">paper</a>] [<a href="https://github.com/Tencent/DepthCrafter">code</a>]  [<a href="https://huggingface.co/spaces/tencent/DepthCrafter">demo</a>] 
    <br/>
    <strong>Topic:</strong> Depth Estimation
    <br/>
    <strong>Session:</strong> Fri 13 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 1 #171
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33862.png?t=1747997885.3577623" title="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/33862.png" alt="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2501.12375" title="Video Depth Anything: Consistent Depth Estimation for Super-Long Videos">
        <strong>Video Depth Anything: Consistent Depth Estimation for Super-Long Videos</strong>
    </a>
    <br/>
    Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang
    <br/>
    [<a href="https://arxiv.org/abs/2501.12375">paper</a>] [<a href="https://github.com/DepthAnything/Video-Depth-Anything">code</a>]  [<a href="https://huggingface.co/spaces/depth-anything/Video-Depth-Anything">demo</a>] 
    <br/>
    <strong>Topic:</strong> Depth Estimation
    <br/>
    <strong>Session:</strong> Sun 15 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 5 #169
</p>
<br/>
<br/>

### explainability and interpretability

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34644.png?t=1748853080.0173087" title="Interpreting Object-level Foundation Models via Visual Precision Search">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/34644.png" alt="Interpreting Object-level Foundation Models via Visual Precision Search" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.16198" title="Interpreting Object-level Foundation Models via Visual Precision Search">
        <strong>üî• Interpreting Object-level Foundation Models via Visual Precision Search</strong>
    </a>
    <br/>
    Ruoyu Chen, Siyuan Liang, Jingzhi Li, Shiming Liu, Maosen Li, Zhen Huang, Hua Zhang, Xiaochun Cao
    <br/>
    [<a href="https://arxiv.org/abs/2411.16198">paper</a>] [<a href="https://github.com/RuoyuChen10/VPS">code</a>]   [<a href="https://colab.research.google.com/github/RuoyuChen10/VPS/blob/main/tutorial/Grounding_DINO_explanation.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Explainability and Interpretability
    <br/>
    <strong>Session:</strong> Sun 15 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 6 #372
</p>
<br/>
<br/>

### gaze-lle

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34852.png?t=1748847619.7422361" title="Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/34852.png" alt="Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.09586" title="Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders">
        <strong>üî• Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders</strong>
    </a>
    <br/>
    Fiona Ryan, Ajay Bati, Sangmin Lee, Daniel Bolya, Judy Hoffman, James M. Rehg
    <br/>
    [<a href="https://arxiv.org/abs/2412.09586">paper</a>] [<a href="https://github.com/fkryan/gazelle">code</a>]  [<a href="https://huggingface.co/spaces/fffiloni/Gaze-LLE">demo</a>] [<a href="https://colab.research.google.com/drive/1TSoyFvNs1-au9kjOZN_fo5ebdzngSPDq?usp=sharing">colab</a>]
    <br/>
    <strong>Topic:</strong> Gaze-LLE
    <br/>
    <strong>Session:</strong> Sun 15 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 6 #98
</p>
<br/>
<br/>

### generative models

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33479.png?t=1748410877.7850628" title="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/33479.png" alt="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.15322" title="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis">
        <strong>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</strong>
    </a>
    <br/>
    Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, Yuki Mitsufuji
    <br/>
    [<a href="https://arxiv.org/abs/2412.15322">paper</a>] [<a href="https://github.com/hkchengrex/MMAudio">code</a>] [<a href="https://youtu.be/YElewUT2M4M">video</a>] [<a href="https://huggingface.co/spaces/hkchengrex/MMAudio">demo</a>] [<a href="https://colab.research.google.com/drive/1TAaXCY2-kPk4xE4PwKB3EqFbSnkUuzZ8?usp=sharing">colab</a>]
    <br/>
    <strong>Topic:</strong> Generative Models
    <br/>
    <strong>Session:</strong> Sun 15 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 6 #260
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32785.png?t=1748780647.7372541" title="SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/32785.png" alt="SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2403.09055" title="SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models">
        <strong>SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models</strong>
    </a>
    <br/>
    Jaerin Lee, Daniel Sungho Jung, Kanggeon Lee, Kyoung Mu Lee
    <br/>
    [<a href="https://arxiv.org/abs/2403.09055">paper</a>] [<a href="https://github.com/ironjr/semantic-draw">code</a>] [<a href="https://www.youtube.com/watch?v=qR06iiaG5nc">video</a>] [<a href="https://huggingface.co/spaces/ironjr/semantic-draw-canvas-sdxl">demo</a>] [<a href="https://colab.research.google.com/github/camenduru/SemanticPalette-jupyter/blob/main/SemanticPalette_jupyter.ipynb">colab</a>]
    <br/>
    <strong>Topic:</strong> Generative Models
    <br/>
    <strong>Session:</strong> Sat 14 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 3 #226
</p>
<br/>
<br/>

### image matching

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34813.png?t=1748519375.4086587" title="MINIMA: Modality Invariant Image Matching">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/34813.png" alt="MINIMA: Modality Invariant Image Matching" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.19412" title="MINIMA: Modality Invariant Image Matching">
        <strong>MINIMA: Modality Invariant Image Matching</strong>
    </a>
    <br/>
    Jiangwei Ren, Xingyu Jiang, Zizhuo Li, Dingkang Liang, Xin Zhou, Xiang Bai
    <br/>
    [<a href="https://arxiv.org/abs/2412.19412">paper</a>] [<a href="https://github.com/LSXI7/MINIMA">code</a>]  [<a href="https://huggingface.co/spaces/lsxi77777/MINIMA">demo</a>] 
    <br/>
    <strong>Topic:</strong> Image Matching
    <br/>
    <strong>Session:</strong> Sun 15 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 5 #190
</p>
<br/>
<br/>

### image vectorization

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34467.png?t=1748720373.8849306" title="Layered Image Vectorization via Semantic Simplification">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/34467.png" alt="Layered Image Vectorization via Semantic Simplification" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2406.05404" title="Layered Image Vectorization via Semantic Simplification">
        <strong>Layered Image Vectorization via Semantic Simplification</strong>
    </a>
    <br/>
    Zhenyu Wang, Jianxi Huang, Zhida Sun, Yuanhao Gong, Daniel Cohen-Or, Min Lu
    <br/>
    [<a href="https://arxiv.org/abs/2406.05404">paper</a>] [<a href="https://github.com/SZUVIZ/layered_vectorization">code</a>] [<a href="https://youtu.be/oUYQKjDlwCw">video</a>]  
    <br/>
    <strong>Topic:</strong> Image Vectorization
    <br/>
    <strong>Session:</strong> Fri 13 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 2 #226
</p>
<br/>
<br/>

### object detection

<p align="left">
    <a href="https://arxiv.org/abs/2412.04234" title="DEIM: DETR with Improved Matching for Fast Convergence">
        <strong>DEIM: DETR with Improved Matching for Fast Convergence</strong>
    </a>
    <br/>
    Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, Xi Shen
    <br/>
    [<a href="https://arxiv.org/abs/2412.04234">paper</a>] [<a href="https://github.com/ShihuaHuang95/DEIM">code</a>]   
    <br/>
    <strong>Topic:</strong> Object Detection
    <br/>
    <strong>Session:</strong> Sat 14 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 3 #432
</p>
<br/>

### object tracking

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/35010.png?t=1748684123.2752578" title="MITracker: Multi-View Integration for Visual Object Tracking">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/35010.png" alt="MITracker: Multi-View Integration for Visual Object Tracking" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2502.20111" title="MITracker: Multi-View Integration for Visual Object Tracking">
        <strong>üî• MITracker: Multi-View Integration for Visual Object Tracking</strong>
    </a>
    <br/>
    Mengjie Xu, Yitao Zhu, Haotian Jiang, Jiaming Li, Zhenrong Shen, Sheng Wang, Haolin Huang, Xinyu Wang, Qing Yang, Han Zhang, Qian Wang
    <br/>
    [<a href="https://arxiv.org/abs/2502.20111">paper</a>] [<a href="https://github.com/XuM007/MITracker">code</a>]   
    <br/>
    <strong>Topic:</strong> Object Tracking
    <br/>
    <strong>Session:</strong> Sun 15 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 6 #98
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33882.png?t=1748536728.1331344" title="Multiple Object Tracking as ID Prediction">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/33882.png" alt="Multiple Object Tracking as ID Prediction" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2403.16848" title="Multiple Object Tracking as ID Prediction">
        <strong>Multiple Object Tracking as ID Prediction</strong>
    </a>
    <br/>
    Ruopeng Gao, Ji Qi, Limin Wang
    <br/>
    [<a href="https://arxiv.org/abs/2403.16848">paper</a>] [<a href="https://github.com/MCG-NJU/MOTIP">code</a>]   
    <br/>
    <strong>Topic:</strong> Object Tracking
    <br/>
    <strong>Session:</strong> Sun 15 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 6 #163
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2501.07256" title="EdgeTAM: On-Device Track Anything Model">
        <strong>EdgeTAM: On-Device Track Anything Model</strong>
    </a>
    <br/>
    Chong Zhou, Chenchen Zhu, Yunyang Xiong, Saksham Suri, Fanyi Xiao, Lemeng Wu, Raghuraman Krishnamoorthi, Bo Dai, Chen Change Loy, Vikas Chandra, Bilge Soran
    <br/>
    [<a href="https://arxiv.org/abs/2501.07256">paper</a>] [<a href="https://github.com/facebookresearch/EdgeTAM">code</a>]  [<a href="https://huggingface.co/spaces/facebook/EdgeTAM">demo</a>] 
    <br/>
    <strong>Topic:</strong> Object Tracking
    <br/>
    <strong>Session:</strong> Sat 14 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 3 #304
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/35133.png?t=1748777099.726237" title="A Distractor-Aware Memory for Visual Object Tracking with SAM2">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/35133.png" alt="A Distractor-Aware Memory for Visual Object Tracking with SAM2" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.17576" title="A Distractor-Aware Memory for Visual Object Tracking with SAM2">
        <strong>A Distractor-Aware Memory for Visual Object Tracking with SAM2</strong>
    </a>
    <br/>
    Jovana Videnovic, Alan Lukezic, Matej Kristan
    <br/>
    [<a href="https://arxiv.org/abs/2411.17576">paper</a>] [<a href="https://github.com/jovanavidenovic/DAM4SAM">code</a>]   
    <br/>
    <strong>Topic:</strong> Object Tracking
    <br/>
    <strong>Session:</strong> Sun 15 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 5 #309
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32888.png?t=1747453263.5318122" title="From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/32888.png" alt="From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.00938" title="From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization">
        <strong>From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization</strong>
    </a>
    <br/>
    Chao Yuan, Guiwei Zhang, Changxiao Ma, Tianyi Zhang, Guanglin Niu
    <br/>
    [<a href="https://arxiv.org/abs/2503.00938">paper</a>] [<a href="https://github.com/yuanc3/Pose2ID">code</a>]   
    <br/>
    <strong>Topic:</strong> Object Tracking
    <br/>
    <strong>Session:</strong> Sun 15 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 5 #190
</p>
<br/>
<br/>

### open-world detection

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/35020.png?t=1748563484.5053573" title="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/35020.png" alt="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2502.07601" title="Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models">
        <strong>üî• Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models</strong>
    </a>
    <br/>
    Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi
    <br/>
    [<a href="https://arxiv.org/abs/2502.07601">paper</a>] [<a href="https://github.com/honda-research-institute/Anomaly-OneVision">code</a>] [<a href="https://www.youtube.com/watch?v=b3-qGTm23eA">video</a>]  
    <br/>
    <strong>Topic:</strong> Open-World Detection
    <br/>
    <strong>Session:</strong> Sat 14 Jun 3 p.m. PDT ‚Äî 5 p.m. PDT Poster Session 4 #435
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32802.png?t=1748865568.2500262" title="Compositional Caching for Training-free Open-vocabulary Attribute Detection">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/32802.png" alt="Compositional Caching for Training-free Open-vocabulary Attribute Detection" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.19145" title="Compositional Caching for Training-free Open-vocabulary Attribute Detection">
        <strong>üî• Compositional Caching for Training-free Open-vocabulary Attribute Detection</strong>
    </a>
    <br/>
    Marco Garosi, Alessandro Conti, Gaowen Liu, Elisa Ricci, Massimiliano Mancini
    <br/>
    [<a href="https://arxiv.org/abs/2503.19145">paper</a>] [<a href="https://github.com/marco-garosi/ComCa">code</a>] [<a href="https://youtu.be/ruHSAGemMa8">video</a>]  
    <br/>
    <strong>Topic:</strong> Open-World Detection
    <br/>
    <strong>Session:</strong> Sat 14 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 3 #426
</p>
<br/>
<br/>

### pose estimation

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/35057.png?t=1748706748.0220559" title="Reconstructing Humans with a Biomechanically Accurate Skeleton">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/35057.png" alt="Reconstructing Humans with a Biomechanically Accurate Skeleton" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.21751" title="Reconstructing Humans with a Biomechanically Accurate Skeleton">
        <strong>üî• Reconstructing Humans with a Biomechanically Accurate Skeleton</strong>
    </a>
    <br/>
    Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos
    <br/>
    [<a href="https://arxiv.org/abs/2503.21751">paper</a>] [<a href="https://github.com/IsshikiHugh/HSMR">code</a>]  [<a href="https://huggingface.co/spaces/IsshikiHugh/HSMR">demo</a>] [<a href="https://colab.research.google.com/drive/1RDA9iKckCDKh_bbaKjO8bQ0-Lv5fw1CB?usp=sharing">colab</a>]
    <br/>
    <strong>Topic:</strong> Pose Estimation
    <br/>
    <strong>Session:</strong> Fri 13 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 2 #91
</p>
<br/>
<br/>

### segmentation

<p align="left">
    <a href="https://arxiv.org/abs/2501.14677" title="MatAnyone: Stable Video Matting with Consistent Memory Propagation">
        <strong>MatAnyone: Stable Video Matting with Consistent Memory Propagation</strong>
    </a>
    <br/>
    Peiqing Yang, Shangchen Zhou, Jixin Zhao, Qingyi Tao, Chen Change Loy
    <br/>
    [<a href="https://arxiv.org/abs/2501.14677">paper</a>] [<a href="https://github.com/pq-yang/MatAnyone">code</a>] [<a href="https://www.youtube.com/watch?v=oih0Zk-UW18">video</a>] [<a href="https://huggingface.co/spaces/PeiqingYang/MatAnyone">demo</a>] 
    <br/>
    <strong>Topic:</strong> Segmentation
    <br/>
    <strong>Session:</strong> Fri 13 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 2 #185
</p>
<br/>

### stereo matching

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34330.png?t=1748714664.9139624" title="FoundationStereo: Zero-Shot Stereo Matching">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/34330.png" alt="FoundationStereo: Zero-Shot Stereo Matching" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2501.09898" title="FoundationStereo: Zero-Shot Stereo Matching">
        <strong>üî• FoundationStereo: Zero-Shot Stereo Matching</strong>
    </a>
    <br/>
    Bowen Wen, Matthew Trepte, Joseph Aribido, Jan Kautz, Orazio Gallo, Stan Birchfield
    <br/>
    [<a href="https://arxiv.org/abs/2501.09898">paper</a>] [<a href="https://github.com/NVlabs/FoundationStereo">code</a>] [<a href="https://www.youtube.com/watch?v=R7RgHxEXB3o">video</a>]  
    <br/>
    <strong>Topic:</strong> Stereo Matching
    <br/>
    <strong>Session:</strong> Fri 13 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 2 #81
</p>
<br/>
<br/>

### video understanding

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32544.png?t=1748596202.019788" title="Towards Universal Soccer Video Understanding">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/32544.png" alt="Towards Universal Soccer Video Understanding" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.01820" title="Towards Universal Soccer Video Understanding">
        <strong>Towards Universal Soccer Video Understanding</strong>
    </a>
    <br/>
    Jiayuan Rao, Haoning Wu, Hao Jiang, Ya Zhang, Yanfeng Wang, Weidi Xie
    <br/>
    [<a href="https://arxiv.org/abs/2412.01820">paper</a>] [<a href="https://github.com/jyrao/UniSoccer">code</a>]   
    <br/>
    <strong>Topic:</strong> Video Understanding
    <br/>
    <strong>Session:</strong> Fri 13 Jun 2 p.m. PDT ‚Äî 4 p.m. PDT Poster Session 2 #185
</p>
<br/>
<br/>

### vision-language models

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32887.png?t=1747896029.4399107" title="FastVLM: Efficient Vision Encoding for Vision Language Models">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/32887.png" alt="FastVLM: Efficient Vision Encoding for Vision Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.13303" title="FastVLM: Efficient Vision Encoding for Vision Language Models">
        <strong>FastVLM: Efficient Vision Encoding for Vision Language Models</strong>
    </a>
    <br/>
    Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari
    <br/>
    [<a href="https://arxiv.org/abs/2412.13303">paper</a>] [<a href="https://github.com/apple/ml-fastvlm">code</a>]   
    <br/>
    <strong>Topic:</strong> Vision-Language Models
    <br/>
    <strong>Session:</strong> Sat 14 Jun 3 p.m. PDT ‚Äî 5 p.m. PDT Poster Session 4 #378
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33073.png?t=1748883064.876014" title="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/33073.png" alt="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2409.17146" title="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models">
        <strong>üî• Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</strong>
    </a>
    <br/>
    Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
    <br/>
    [<a href="https://arxiv.org/abs/2409.17146">paper</a>]   [<a href="https://huggingface.co/spaces/akhaliq/Molmo-7B-D-0924">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision-Language Models
    <br/>
    <strong>Session:</strong> Fri 13 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 1 #80
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34510.png?t=1748805761.17" title="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/34510.png" alt="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.17646" title="SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation">
        <strong>üî• SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation</strong>
    </a>
    <br/>
    Claudia Cuttano, Gabriele Trivigno, Gabriele Rosi, Carlo Masone, Giuseppe Averta
    <br/>
    [<a href="https://arxiv.org/abs/2411.17646">paper</a>] [<a href="https://github.com/ClaudiaCuttano/SAMWISE">code</a>] [<a href="https://youtu.be/OL3xvzFyXCc">video</a>]  
    <br/>
    <strong>Topic:</strong> Vision-Language Models
    <br/>
    <strong>Session:</strong> Fri 13 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 1 #308
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34048.png?t=1748839405.5943303" title="VisionArena: 230K Real World User-VLM Conversations with Preference Labels">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/34048.png" alt="VisionArena: 230K Real World User-VLM Conversations with Preference Labels" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.08687" title="VisionArena: 230K Real World User-VLM Conversations with Preference Labels">
        <strong>VisionArena: 230K Real World User-VLM Conversations with Preference Labels</strong>
    </a>
    <br/>
    Christopher Chou, Lisa Dunlap, Koki Mashita, Krishna Mandal, Trevor Darrell, Ion Stoica, Joseph E. Gonzalez, Wei-Lin Chiang
    <br/>
    [<a href="https://arxiv.org/abs/2412.08687">paper</a>]   [<a href="https://huggingface.co/datasets/lmarena-ai/VisionArena-Battle">demo</a>] 
    <br/>
    <strong>Topic:</strong> Vision-Language Models
    <br/>
    <strong>Session:</strong> Fri 13 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 1 #353
</p>
<br/>
<br/>

### visual agents

<p align="left">
    <a href="https://arxiv.org/abs/2502.13130" title="Magma: A Foundation Model for Multimodal AI Agents">
        <strong>Magma: A Foundation Model for Multimodal AI Agents</strong>
    </a>
    <br/>
    Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, Jianfeng Gao
    <br/>
    [<a href="https://arxiv.org/abs/2502.13130">paper</a>] [<a href="https://github.com/microsoft/Magma">code</a>] [<a href="https://www.youtube.com/watch?v=SbfzvUU5yM8">video</a>] [<a href="https://huggingface.co/spaces/microsoft/Magma-UI">demo</a>] 
    <br/>
    <strong>Topic:</strong> Visual Agents
    <br/>
    <strong>Session:</strong> Sat 14 Jun 8:30 a.m. PDT ‚Äî 10:30 a.m. PDT Poster Session 3 #340
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33472.png?t=1748798588.1133444" title="ShowUI: One Vision-Language-Action Model for GUI Visual Agent">
        <img src="https://storage.googleapis.com/com-roboflow-marketing/cvpr-2025-posters/33472.png" alt="ShowUI: One Vision-Language-Action Model for GUI Visual Agent" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2411.17465" title="ShowUI: One Vision-Language-Action Model for GUI Visual Agent">
        <strong>ShowUI: One Vision-Language-Action Model for GUI Visual Agent</strong>
    </a>
    <br/>
    Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, Mike Zheng Shou
    <br/>
    [<a href="https://arxiv.org/abs/2411.17465">paper</a>] [<a href="https://github.com/showlab/ShowUI">code</a>]  [<a href="https://huggingface.co/spaces/showlab/ShowUI">demo</a>] 
    <br/>
    <strong>Topic:</strong> Visual Agents
    <br/>
    <strong>Session:</strong> Sat 14 Jun 3 p.m. PDT ‚Äî 5 p.m. PDT Poster Session 4 #352
</p>
<br/>
<br/>

<!--- AUTOGENERATED_PAPERS_LIST -->

## ü¶∏ contribution

We would love your help in making this repository even better! If you know of an amazing
paper that isn't listed here, or if you have any suggestions for improvement, feel free
to open an
[issue](https://github.com/SkalskiP/top-cvpr-2025-papers/issues)
or submit a
[pull request](https://github.com/SkalskiP/top-cvpr-2025-papers/pulls).
