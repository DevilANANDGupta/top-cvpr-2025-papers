![visitor badge](https://visitor-badge.laobi.icu/badge?page_id=SkalskiP.top-cvpr-2025-papers)

<div align="center">
  <h1 align="center">top CVPR 2025 papers</h1>
  <a href="https://github.com/SkalskiP/top-cvpr-2023-papers">2023</a> | <a href="https://github.com/SkalskiP/top-cvpr-2024-papers">2024</a> | <a href="https://github.com/SkalskiP/top-cvpr-2025-papers">2025</a>
</div>

## üëã hello

Computer Vision and Pattern Recognition is a massive conference. In **2025** alone,
**13,008** papers were submitted, and **2,878** were accepted. I created this repository
to help you search for cr√®me de la cr√®me of CVPR publications. If the paper you are
looking for is not on my short list, take a peek at the full
[list](https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers) of accepted papers.

## üóûÔ∏è papers and posters

*üî• - highlighted papers*

<!--- AUTOGENERATED_PAPERS_LIST -->
<!---
   WARNING: DO NOT EDIT THIS LIST MANUALLY. IT IS AUTOMATICALLY GENERATED.
   HEAD OVER TO https://github.com/SkalskiP/top-cvpr-2025-papers/blob/master/CONTRIBUTING.md FOR MORE DETAILS ON HOW TO MAKE CHANGES PROPERLY.
-->

<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32554.png?t=1748195633.2703488" title="UniK3D: Universal Camera Monocular 3D Estimation">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32554.png?t=1748195633.2703488" alt="UniK3D: Universal Camera Monocular 3D Estimation" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.16591" title="UniK3D: Universal Camera Monocular 3D Estimation">
        <strong>UniK3D: Universal Camera Monocular 3D Estimation</strong>
    </a>
    <br/>
    Luigi Piccinelli, Christos Sakaridis, Mattia Segu, Yung-Hsu Yang, Siyuan Li, Wim Abbeloos, Luc Van Gool
    <br/>
    [<a href="https://arxiv.org/abs/2503.16591">paper</a>] [<a href="https://github.com/lpiccinelli-eth/UniK3D">code</a>]  [<a href="https://huggingface.co/spaces/lpiccinelli/UniK3D-demo">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #80
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2409.17146" title="Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models">
        <strong>üî• Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models</strong>
    </a>
    <br/>
    Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, Aniruddha Kembhavi
    <br/>
    [<a href="https://arxiv.org/abs/2409.17146">paper</a>]   [<a href="https://huggingface.co/spaces/akhaliq/Molmo-7B-D-0924">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #80
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32887.png?t=1747896029.4399107" title="FastVLM: Efficient Vision Encoding for Vision Language Models">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32887.png?t=1747896029.4399107" alt="FastVLM: Efficient Vision Encoding for Vision Language Models" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.13303" title="FastVLM: Efficient Vision Encoding for Vision Language Models">
        <strong>FastVLM: Efficient Vision Encoding for Vision Language Models</strong>
    </a>
    <br/>
    Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, Hadi Pouransari
    <br/>
    [<a href="https://arxiv.org/abs/2412.13303">paper</a>] [<a href="https://github.com/apple/ml-fastvlm">code</a>]   
    <br/>
    <strong>Session:</strong> Sun 15 Jun midnight CEST ‚Äî 2 a.m. CEST
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33479.png?t=1748410877.7850628" title="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33479.png?t=1748410877.7850628" alt="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.15322" title="MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis">
        <strong>MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis</strong>
    </a>
    <br/>
    Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, Yuki Mitsufuji
    <br/>
    [<a href="https://arxiv.org/abs/2412.15322">paper</a>] [<a href="https://github.com/hkchengrex/MMAudio">code</a>] [<a href="https://youtu.be/YElewUT2M4M">video</a>] [<a href="https://huggingface.co/spaces/hkchengrex/MMAudio">demo</a>] [<a href="https://colab.research.google.com/drive/1TAaXCY2-kPk4xE4PwKB3EqFbSnkUuzZ8?usp=sharing">colab</a>]
    <br/>
    <strong>Session:</strong> Sun 15 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST #260
</p>
<br/>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34813.png?t=1748519375.4086587" title="MINIMA: Modality Invariant Image Matching">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34813.png?t=1748519375.4086587" alt="MINIMA: Modality Invariant Image Matching" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2412.19412" title="MINIMA: Modality Invariant Image Matching">
        <strong>MINIMA: Modality Invariant Image Matching</strong>
    </a>
    <br/>
    Jiangwei Ren, Xingyu Jiang, Zizhuo Li, Dingkang Liang, Xin Zhou, Xiang Bai
    <br/>
    [<a href="https://arxiv.org/abs/2412.19412">paper</a>] [<a href="https://github.com/LSXI7/MINIMA">code</a>]  [<a href="https://huggingface.co/spaces/lsxi77777/MINIMA">demo</a>] 
    <br/>
    <strong>Session:</strong> Sun 15 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST #190
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2412.04234" title="DEIM: DETR with Improved Matching for Fast Convergence">
        <strong>DEIM: DETR with Improved Matching for Fast Convergence</strong>
    </a>
    <br/>
    Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, Xi Shen
    <br/>
    [<a href="https://arxiv.org/abs/2412.04234">paper</a>] [<a href="https://github.com/ShihuaHuang95/DEIM">code</a>]   
    <br/>
    <strong>Session:</strong> Sat 14 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST
</p>
<br/>


<p align="left">
    <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33969.png?t=1748740040.9726639" title="VGGT: Visual Geometry Grounded Transformer">
        <img src="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/33969.png?t=1748740040.9726639" alt="VGGT: Visual Geometry Grounded Transformer" width="400px" align="left" />
    </a>
    <a href="https://arxiv.org/abs/2503.11651" title="VGGT: Visual Geometry Grounded Transformer">
        <strong>üî• VGGT: Visual Geometry Grounded Transformer</strong>
    </a>
    <br/>
    Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, David Novotny
    <br/>
    [<a href="https://arxiv.org/abs/2503.11651">paper</a>] [<a href="https://github.com/facebookresearch/vggt">code</a>] [<a href="https://youtu.be/7ZYwJEpCUUA">video</a>] [<a href="https://huggingface.co/spaces/facebook/vggt">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST
</p>
<br/>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2502.13130" title="Magma: A Foundation Model for Multimodal AI Agents">
        <strong>Magma: A Foundation Model for Multimodal AI Agents</strong>
    </a>
    <br/>
    Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, Yuquan Deng, Lars Liden, Jianfeng Gao
    <br/>
    [<a href="https://arxiv.org/abs/2502.13130">paper</a>] [<a href="https://github.com/microsoft/Magma">code</a>] [<a href="https://www.youtube.com/watch?v=SbfzvUU5yM8">video</a>] [<a href="https://huggingface.co/spaces/microsoft/Magma-UI">demo</a>] 
    <br/>
    <strong>Session:</strong> Sat 14 Jun 5:30 p.m. CEST ‚Äî 7:30 p.m. CEST
</p>
<br/>


<p align="left">
    <a href="https://arxiv.org/abs/2501.14677" title="MatAnyone: Stable Video Matting with Consistent Memory Propagation">
        <strong>MatAnyone: Stable Video Matting with Consistent Memory Propagation</strong>
    </a>
    <br/>
    Peiqing Yang, Shangchen Zhou, Jixin Zhao, Qingyi Tao, Chen Change Loy
    <br/>
    [<a href="https://arxiv.org/abs/2501.14677">paper</a>] [<a href="https://github.com/pq-yang/MatAnyone">code</a>] [<a href="https://www.youtube.com/watch?v=oih0Zk-UW18">video</a>] [<a href="https://huggingface.co/spaces/PeiqingYang/MatAnyone">demo</a>] 
    <br/>
    <strong>Session:</strong> Fri 13 Jun 11 p.m. CEST ‚Äî 1 a.m. CEST
</p>
<br/>

<!--- AUTOGENERATED_PAPERS_LIST -->

## ü¶∏ contribution

We would love your help in making this repository even better! If you know of an amazing
paper that isn't listed here, or if you have any suggestions for improvement, feel free
to open an
[issue](https://github.com/SkalskiP/top-cvpr-2025-papers/issues)
or submit a
[pull request](https://github.com/SkalskiP/top-cvpr-2025-papers/pulls).
